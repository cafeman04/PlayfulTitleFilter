{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JCp-fP-rsjc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d6917a-fbea-474c-fdcc-6488bf168f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import hdbscan\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DZJOtRJruMZW"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/All_capped_keywords.csv')\n",
        "titles = df['title']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the embeddings for all paper titles"
      ],
      "metadata": {
        "id": "VcVuc9mA1C4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "a917dea8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(titles, batch_size=64)\n",
        "np.save(\"embeddings.npy\", embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach #1 - Finding clusters"
      ],
      "metadata": {
        "id": "meHDT4I21Ps_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reducing embedding dimensions for better speed, but not needed\n",
        "from sklearn.decomposition import PCA\n",
        "embeddings = np.load(\"embeddings.npy\")\n",
        "pca = PCA(n_components=50)\n",
        "embeddings_reduced = pca.fit_transform(embeddings)\n",
        "np.save(\"embeddings_reduced50.npy\", embeddings_reduced)"
      ],
      "metadata": {
        "id": "tK3Lr2Yy0S7N"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4RhFzMvRuI4",
        "outputId": "777b1db4-6ab7-4832-efd2-706315d3a85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Find the clusters in the reduced embeddings\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=30, metric='euclidean')\n",
        "embeddings_reduced = np.load(\"embeddings_reduced50.npy\")\n",
        "labels = clusterer.fit_predict(embeddings_reduced)\n",
        "np.save(\"labels50.npy\", labels)\n",
        "labels = np.load(\"labels50.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating semantic clusters at first seemed like a wise idea as I believed a cluster of silly / funny titles would've appeared in one of them, however this was not the case. I realized I may have to change gears and have to nudge the \"filtering\" towards titles that I specifically want, which are the silly/funny ones."
      ],
      "metadata": {
        "id": "klN67Fqj4uk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_HviYb_O69ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, I tried looking into seeing if the highest outliers in the embeddings were kinds of titles I was looking for, however it was not the case. The outliers just mean that they were simply very different than other titles and don't share any similarity, which does not imply they will turn out silly. The outliers may be unique, however it will allow the titles to only contain lots of technical jargon, which is the the opposite of what I wanted.\n"
      ],
      "metadata": {
        "id": "qIVc58Kf5K4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t10 = np.argsort(clusterer.outlier_scores_)[:10]\n",
        "for i in t10:\n",
        "  print(titles[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpFY199NHMFN",
        "outputId": "8dba831b-1671-4692-dcc6-1376aac964a7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model\n",
            "Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems\n",
            "PeriScope: An Effective Probing and Fuzzing Framework for the Hardware-OS Boundary\n",
            "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars\n",
            "QuAC: Question Answering in Context\n",
            "SpecDoctor: Differential Fuzz Testing to Find Transient Execution Vulnerabilities\n",
            "Learning Unsupervised Metaformer for Anomaly Detection\n",
            "Scene Context-Aware Salient Object Detection\n",
            "Declarative Question Answering over Knowledge Bases containing Natural Language Text with Answer Set Programming\n",
            "Making Decision like Human: Joint Aspect Category Sentiment Analysis and Rating Prediction with Fine-to-Coarse Reasoning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach #2 - Cosine similarity w/ seeds"
      ],
      "metadata": {
        "id": "LldyNsjB7Cnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second approach I tried out was picking out ideal titles from a random selection in the large set of titles. I looked for silly, unique, and interesting titles.\n",
        "\n",
        "At first, I kept the entire title for these selected seed titles, however there were many titles found \"similar\" to the seeds that only contained technical words and nothing eye catching. Then, I chose to eliminate the technical portions from the seeds and only keep the interesting and silly portions to help nudge the filtered titles to my desired output, which did seem to help. I also added some hand-written seeds that I would see as eye catching in a paper title.\n"
      ],
      "metadata": {
        "id": "A5k-W3TR7clU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [\n",
        "    \"Goodbye Python. Goodbye Java.\",\n",
        "    \"Adios array bounds checking and godspeed garbage collector\",\n",
        "    \"Auf wiedersehen exceptions\",\n",
        "    \"So long OO-programming… whatever you are\",\n",
        "    \"Adieu inferred polymorphic types, higher-order functions, rich built-in data\",\n",
        "    \"My Brain is a Bag of Spiking Neurons\",\n",
        "    \"A Tale of Two Dropouts\",\n",
        "    \"Do Androids Dream of Differentiable Sheep?\",\n",
        "    \"Neural Networks Say the Darndest Things\",\n",
        "    \"Herding Cats with Gradient Descent\",\n",
        "    \"Revenge of the Overfitting\",\n",
        "    \"Attack of the Vanishing Gradients\",\n",
        "    \"Lord of the Embeddings\",\n",
        "    \"Fifty Shades of Backpropagation\",\n",
        "    \"Harry Potter and the Order of the Convolution\",\n",
        "    \"Raiders of the Lost Gradient\",\n",
        "    \"Mission Impossible: Generalization Protocol\",\n",
        "    \"The Matrix Reparameterized\",\n",
        "    \"Much Ado About Attention\",\n",
        "    \"The Good, the Bad, and the Overfitted\",\n",
        "    \"Catch Me If You GAN\",\n",
        "    \"To Err is Human, to Overfit Divine\",\n",
        "    \"Fast and Curious\",\n",
        "    \"The Curious Case of the Exploding Gradient\",\n",
        "    \"Don’t Overfit Me Like That\",\n",
        "    \"Dude, Where’s My GPU?\",\n",
        "    \"Lost in Latent Space\",\n",
        "    \"How I Learned to Stop Worrying and Love the Noise\",\n",
        "    \"Fear and Loathing in Optimization\",\n",
        "    \"Alice in Embeddingland\",\n",
        "    \"Lord of the Weights\",\n",
        "    \"Detect Me If You Can\",\n",
        "    \"Cookie Swap Party\",\n",
        "    \"Four-Dimensional Shopping Mall\",\n",
        "    \"Are Two Heads Better Than One?\",\n",
        "    \"Twin Peaks\",\n",
        "    \"It's Almost Like They’re Trying to Hide It\",\n",
        "    \"One Picture is Worth a Thousand Words?\",\n",
        "    \"I Would Not Plant Apple Trees If the World Will Be Wiped\",\n",
        "    \"The One Hundred Year Web\",\n",
        "    \"The Chameleon Attack\",\n",
        "    \"The Paradigm-Shift of Social Spambots\",\n",
        "    \"Are All Successful Communities Alike?\",\n",
        "    \"Fighting Against Deepfake: Patch & Pair\",\n",
        "    \"The Spread of Physical Activity Through Social Networks\",\n",
        "    \"FastSNG: The Fastest Social Network Dataset Generator\",\n",
        "    \"Plumber\",\n",
        "    \"Genre Differences of Song Lyrics\",\n",
        "    \"Aspect of Blame\",\n",
        "    \"Dude, Where’s My Gradient?\"\n",
        "]\n",
        "print(len(seeds))\n",
        "\n",
        "\n",
        "#Finding the embeddings for the seeds\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(seeds, batch_size=64, show_progress_bar=True)\n",
        "np.save(\"seedEmbedding.npy\", embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "942787c928a248bf931936ef633ef04f",
            "7192e37c7a604742900d46235acd2200",
            "24bf4077ae7941e391afcb45b19edeb5",
            "d441fdd8080e4f89afa59d2055961b26",
            "4d98869e4564453bb104e9f617187213",
            "7ea059fb25cd44f5970bd4fd289d1311",
            "4875f0b0be564f6385161fdfdd7c7436",
            "83b869373f794c9bb6404d8a789347e0",
            "6bff571f59f54a5c86ef889dcb1a6c90",
            "1057878a0c3d42c399bd1cf5193c0d91",
            "6b1042898ea44588ad0be772843bbf80"
          ]
        },
        "id": "H7ZsGL4iSmIi",
        "outputId": "6ac76db2-ffcd-40c0-f68c-ca1d76e0d843"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "942787c928a248bf931936ef633ef04f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.util import cos_sim\n",
        "import torch\n",
        "all_embeddings = np.load(\"embeddings.npy\")\n",
        "seed_embeddings = np.load(\"seedEmbedding.npy\")\n",
        "sims = cos_sim(all_embeddings, seed_embeddings)  # shape: [num_titles, num_seeds]\n",
        "print(sims.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwHVNNOoVpjd",
        "outputId": "55e99378-3af7-4a99-b59b-4e32ebff6921"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([91919, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length = 0\n",
        "#Finding the titles that have a similarity of above a threshold of 0.5\n",
        "for t in range(sims.shape[0]):\n",
        "    best_sim = torch.max(sims[t]).item()\n",
        "    if best_sim >= 0.5:\n",
        "        print(f\"{titles[t]} | {best_sim:.3f}\")\n",
        "        length += 1\n",
        "\n",
        "print(f\"{length} titles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7PrrkvhqdW4",
        "outputId": "afba7bcb-3544-4eae-8a59-2e44165cbcee"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memorization Weights for Instance Reweighting in Adversarial Training | 0.503\n",
            "Invariant Information Bottleneck for Domain Generalization | 0.566\n",
            "Online DR-Submodular Maximization: Minimizing Regret and Constraint Violation | 0.534\n",
            "A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to Regularization | 0.516\n",
            "The HSIC Bottleneck: Deep Learning without Back-Propagation | 0.536\n",
            "Safe Online Convex Optimization with Unknown Linear Safety Constraints | 0.511\n",
            "Dropout Model Evaluation in MOOCs | 0.547\n",
            "Beyond Unfolding: Exact Recovery of Latent Convex Tensor Decomposition Under Reshuffling | 0.506\n",
            "Certifying the True Error: Machine Learning in Coq with Verified Generalization Guarantees | 0.525\n",
            "Learning Nonlinear Dynamics in Efficient, Balanced Spiking Networks Using Local Plasticity Rules | 0.506\n",
            "Auto-Encoding Transformations in Reparameterized Lie Groups for Unsupervised Learning | 0.505\n",
            "The Counterfactual NESS Definition of Causation | 0.503\n",
            "Understanding Catastrophic Overfitting in Single-step Adversarial Training | 0.624\n",
            "When Neural Networks Fail to Generalize? A Model Sensitivity Perspective | 0.566\n",
            "Adversarial Dropout for Supervised and Semi-supervised Learning | 0.512\n",
            "Balancing spreads of influence in a social network | 0.526\n",
            "A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories | 0.500\n",
            "A Risk-Sensitive Approach to Policy Optimization | 0.576\n",
            "Understanding Dropouts in MOOCs | 0.613\n",
            "Demystify the Gravity Well in the Optimization Landscape (Student Abstract) | 0.511\n",
            "Bounding Regret in Empirical Games | 0.507\n",
            "Towards Formal Definitions of Blameworthiness, Intention, and Moral Responsibility | 0.660\n",
            "POEM: Polarization of Embeddings for Domain-Invariant Representations | 0.540\n",
            "Uncertainty-Aware Search Framework for Multi-Objective Bayesian Optimization | 0.517\n",
            "The Perils of Learning Before Optimizing | 0.639\n",
            "Improving Gradient Flow with Unrolled Highway Expectation Maximization | 0.522\n",
            "Group-Wise Dynamic Dropout Based on Latent Semantic Variations | 0.546\n",
            "An Interactive Regret-Based Genetic Algorithm for Solving Multi-Objective Combinatorial Optimization Problems | 0.524\n",
            "Deep Spiking Neural Networks with High Representation Similarity Model Visual Pathways of Macaque and Mouse | 0.512\n",
            "Gradient Boosts the Approximate Vanishing Ideal | 0.690\n",
            "Latent Space Explanation by Intervention | 0.753\n",
            "Implicit Stochastic Gradient Descent for Training Physics-informed Neural Networks | 0.502\n",
            "Training Spiking Neural Networks with Accumulated Spiking Flow | 0.593\n",
            "Clouseau: Generating Communication Protocols from Commitments | 0.517\n",
            "A Fast Algorithm to Compute Maximum k-Plexes in Social Network Analysis | 0.538\n",
            "Randomized Strategies for Robust Combinatorial Optimization | 0.515\n",
            "Policy Optimization with Model-based Explorations | 0.507\n",
            "Improving First-Order Optimization Algorithms (Student Abstract) | 0.523\n",
            "Group Activity Selection on Social Networks | 0.593\n",
            "Least General Generalizations in Description Logic: Verification and Existence | 0.514\n",
            "Backpropagation-Free Deep Learning with Recursive Local Representation Alignment | 0.507\n",
            "Exploratory Combinatorial Optimization with Reinforcement Learning | 0.510\n",
            "Direct Training for Spiking Neural Networks: Faster, Larger, Better | 0.561\n",
            "Certifiable Out-of-Distribution Generalization | 0.609\n",
            "Well-classified Examples are Underestimated in Classification with Deep Neural Networks | 0.520\n",
            "VECA: A Method for Detecting Overfitting in Neural Networks (Student Abstract) | 0.501\n",
            "Sharp Restricted Isometry Property Bounds for Low-rank Matrix Recovery Problems with Corrupted Measurements | 0.533\n",
            "Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection | 0.521\n",
            "Can You Answer This? - Exploring Zero-Shot QA Generalization Capabilities in Large Language Models (Student Abstract) | 0.504\n",
            "Guided Dropout | 0.648\n",
            "Accountability Layers: Explaining Complex System Failures by Parts | 0.507\n",
            "Delving into Variance Transmission and Normalization: Shift of Average Gradient Makes the Network Collapse | 0.519\n",
            "Polynomial Optimization Methods for Matrix Factorization | 0.504\n",
            "Continual Learning with Scaled Gradient Projection | 0.530\n",
            "Neural Networks Classify through the Class-Wise Means of Their Representations | 0.524\n",
            "Beyond NaN: Resiliency of Optimization Layers in The Face of Infeasibility | 0.645\n",
            "Biologically Plausible Sequence Learning with Spiking Neural Networks | 0.571\n",
            "Explaining Neural Matrix Factorization with Gradient Rollback | 0.505\n",
            "Learning Invariant Representations using Inverse Contrastive Loss | 0.523\n",
            "Backforward Propagation (Student Abstract) | 0.585\n",
            "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win | 0.520\n",
            "Interpretation of Neural Networks is Fragile | 0.534\n",
            "Scalable and Explainable 1-Bit Matrix Completion via Graph Signal Learning | 0.507\n",
            "JFB: Jacobian-Free Backpropagation for Implicit Networks | 0.551\n",
            "StarSpace: Embed All The Things! | 0.527\n",
            "Problem Difficulty and the Phase Transition in Heuristic Search | 0.500\n",
            "On Online Optimization: Dynamic Regret Analysis of Strongly Convex and Smooth Problems | 0.543\n",
            "Present-Biased Optimization | 0.527\n",
            "Fastened CROWN: Tightened Neural Network Robustness Certificates | 0.519\n",
            "Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking | 0.530\n",
            "Blameworthiness in Strategic Games | 0.556\n",
            "Dynamic Thresholding and Pruning for Regret Minimization | 0.513\n",
            "Local and Global Linear Convergence of General Low-Rank Matrix Recovery Problems | 0.509\n",
            "Modelling and Solving Online Optimisation Problems | 0.530\n",
            "Numerical Optimization to AI, and Back | 0.530\n",
            "Adversarial Dropout for Recurrent Neural Networks | 0.526\n",
            "Constrained Reinforcement Learning in Hard Exploration Problems | 0.504\n",
            "Deep Spiking Neural Network with Neural Oscillation and Spike-Phase Information | 0.576\n",
            "GANs Unplugged | 0.516\n",
            "The Goldilocks zone: Towards better understanding of neural network loss landscapes | 0.574\n",
            "Evolving Spiking Circuit Motifs Using Weight Agnostic Neural Networks | 0.535\n",
            "Fast and Deep Graph Neural Networks | 0.501\n",
            "TransConv: Relationship Embedding in Social Networks | 0.526\n",
            "Safeguarded Learned Convex Optimization | 0.500\n",
            "Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption | 0.502\n",
            "Forming Probably Stable Communities with Limited Interactions | 0.530\n",
            "Risk-Sensitive Submodular Optimization | 0.533\n",
            "Exploring Temporal Information Dynamics in Spiking Neural Networks | 0.549\n",
            "AdaLoss: A computationally-efficient and provably convergent adaptive gradient method | 0.568\n",
            "Deterministic Value-Policy Gradients | 0.503\n",
            "Going Deeper With Directly-Trained Larger Spiking Neural Networks | 0.534\n",
            "High Rank Matrix Completion With Side Information | 0.557\n",
            "Maximizing Influence Spread through a Dynamic Social Network (Student Abstract) | 0.502\n",
            "PegasusN: A Scalable and Versatile Graph Mining System | 0.524\n",
            "Dropout is NOT All You Need to Prevent Gradient Leakage | 0.532\n",
            "Reinforcement Learning under Threats | 0.543\n",
            "Selfish Creation of Social Networks | 0.500\n",
            "SpikeConverter: An Efficient Conversion Framework Zipping the Gap between Artificial Neural Networks and Spiking Neural Networks | 0.565\n",
            "Improved Algorithms for Conservative Exploration in Bandits | 0.515\n",
            "Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules | 0.518\n",
            "Obtaining Faithful Interpretations from Compositional Neural Networks | 0.510\n",
            "Is Attention Interpretable? | 0.506\n",
            "Finding the Pillars of Strength for Multi-Head Attention | 0.544\n",
            "CAME: Confidence-guided Adaptive Memory Efficient Optimization | 0.527\n",
            "One Embedder, Any Task: Instruction-Finetuned Text Embeddings | 0.536\n",
            "Contextual Embeddings: When Are They Worth It? | 0.522\n",
            "A Gentle Introduction to Deep Nets and Opportunities for the Future | 0.503\n",
            "Generalizing Backpropagation for Gradient-Based Interpretability | 0.529\n",
            "Extracting Commonsense Properties from Embeddings with Limited Human Guidance | 0.516\n",
            "Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae | 0.516\n",
            "Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color | 0.510\n",
            "Overcoming a Theoretical Limitation of Self-Attention | 0.532\n",
            "A Mixture of h - 1 Heads is Better than h Heads | 0.623\n",
            "Is Attention Explanation? An Introduction to the Debate | 0.526\n",
            "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario | 0.545\n",
            "Contrasting Exploration in Parameter and Action Space: A Zeroth-Order Optimization Perspective | 0.536\n",
            "Variance reduction properties of the reparameterization trick | 0.584\n",
            "Probabilistic sequential matrix factorization | 0.508\n",
            "Experimental Design for Regret Minimization in Linear Bandits | 0.505\n",
            "LocoProp: Enhancing BackProp via Local Loss Optimization | 0.503\n",
            "Masked Training of Neural Networks with Partial Gradients | 0.537\n",
            "Revisiting the Landscape of Matrix Factorization | 0.565\n",
            "Vanishing Curvature in Randomly Initialized Deep ReLU Networks | 0.507\n",
            "AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization | 0.510\n",
            "Robust Matrix Completion from Quantized Observations | 0.579\n",
            "PAC-Bayesian Learning of Optimization Algorithms | 0.528\n",
            "Complex Momentum for Optimization in Games | 0.518\n",
            "Minimax Estimation of Laplacian Constrained Precision Matrices | 0.505\n",
            "Robust Optimisation Monte Carlo | 0.519\n",
            "Minimax Rank-$1$ Matrix Factorization | 0.517\n",
            "Towards Memory-Friendly Deterministic Incremental Gradient Method | 0.519\n",
            "On the Generalization Properties of Adversarial Training | 0.512\n",
            "A Fast Algorithm for Separated Sparsity via Perturbed Lagrangians | 0.504\n",
            "An Information-Theoretic Route from Generalization in Expectation to Generalization in Probability | 0.539\n",
            "Bayesian optimisation under uncertain inputs | 0.523\n",
            "Gradient Diversity: a Key Ingredient for Scalable Distributed Learning | 0.529\n",
            "Riemannian accelerated gradient methods via extrapolation | 0.544\n",
            "qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization | 0.508\n",
            "Bures-Wasserstein Barycenters and Low-Rank Matrix Recovery | 0.558\n",
            "Learning with Gradient Descent and Weakly Convex Losses | 0.523\n",
            "Noise Regularizes Over-parameterized Rank One Matrix Recovery, Provably | 0.602\n",
            "Randomized Stochastic Gradient Descent Ascent | 0.522\n",
            "Understanding Gradient Clipping In Incremental Gradient Methods | 0.522\n",
            "Laplacian Constrained Precision Matrix Estimation: Existence and High Dimensional Consistency | 0.550\n",
            "Reconstructing Training Data from Model Gradient, Provably | 0.567\n",
            "Learning to Generalize Provably in Learning to Optimize | 0.541\n",
            "Mixed Strategies for Robust Optimization of Unknown Objectives | 0.562\n",
            "Low-rank regularization and solution uniqueness in over-parameterized matrix sensing | 0.561\n",
            "Mode estimation on matrix manifolds: Convergence and robustness | 0.540\n",
            "Conditional Gradients for the Approximately Vanishing Ideal | 0.716\n",
            "Generalizing the theory of cooperative inference | 0.540\n",
            "Nearest Neighbour Based Estimates of Gradients: Sharp Nonasymptotic Bounds and Applications | 0.511\n",
            "Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods | 0.507\n",
            "Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity | 0.503\n",
            "Solving the Robust Matrix Completion Problem via a System of Nonlinear Equations | 0.587\n",
            "Locally Accelerated Conditional Gradients | 0.586\n",
            "How and When Random Feedback Works: A Case Study of Low-Rank Matrix Factorization | 0.518\n",
            "Geometrically Enriched Latent Spaces | 0.526\n",
            "Predicting the utility of search spaces for black-box optimization: a simple, budget-aware approach | 0.550\n",
            "Active Positive Semidefinite Matrix Completion: Algorithms, Theory and Applications | 0.517\n",
            "Learning to Optimize with Stochastic Dominance Constraints | 0.511\n",
            "A Study of Condition Numbers for First-Order Optimization | 0.502\n",
            "On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow | 0.501\n",
            "Lifelong Optimization with Low Regret | 0.617\n",
            "Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models | 0.506\n",
            "The Sylvester Graphical Lasso (SyGlasso) | 0.504\n",
            "KickStarter: Fast and Accurate Computations on Streaming Graphs via Trimmed Approximations | 0.514\n",
            "Mapping Very Large Scale Spiking Neuron Network to Neuromorphic Hardware | 0.559\n",
            "HMC: Model Checking for Hardware Memory Models | 0.501\n",
            "FreeGuard: A Faster Secure Heap Allocator | 0.521\n",
            "SSR'19: The 5th Conference on Security Standardisation Research | 0.508\n",
            "POISED: Spotting Twitter Spam Off the Beaten Paths | 0.541\n",
            "No-Match Attacks and Robust Partnering Definitions: Defining Trivial Attacks for Security Protocols is Not Trivial | 0.529\n",
            "Rewriting History: Changing the Archived Web from the Present | 0.554\n",
            "Poster: Panacea --- Stateless and Non-Interactive Oblivious RAM | 0.523\n",
            "Latent Backdoor Attacks on Deep Neural Networks | 0.504\n",
            "You Call This Archaeology? Evaluating Web Archives for Reproducible Web Security Measurements | 0.534\n",
            "A Run a Day Won't Keep the Hacker Away: Inference Attacks on Endpoint Privacy Zones in Fitness Tracking Social Networks | 0.532\n",
            "walk2friends: Inferring Social Links from Mobility Profiles | 0.514\n",
            "Sharing Communities: The Good, the Bad, and the Ugly | 0.543\n",
            "When Fitness Meets Social Networks: Investigating Fitness Tracking and Social Practices on WeRun | 0.692\n",
            "StoryMap: Using Social Modeling and Self-Modeling to Support Physical Activity Among Families of Low-SES Backgrounds | 0.577\n",
            "Avoiding the Turing Tarpit: Learning Conversational Programming by Starting from Code’s Purpose | 0.516\n",
            "When Personal Tracking Becomes Social: Examining the Use of Instagram for Healthy Eating | 0.505\n",
            "Overcoming Algorithm Aversion: A Comparison between Process and Outcome Control | 0.555\n",
            "Evaluating CoBlox: A Comparative Study of Robotics Programming Environments for Adult Novices | 0.505\n",
            "Examining the Demand for Spam: Who Clicks? | 0.526\n",
            "OPTIMISM: Enabling Collaborative Implementation of Domain Specific Metaheuristic Optimization | 0.543\n",
            "GPkit: A Human-Centered Approach to Convex Optimization in Engineering Design | 0.524\n",
            "Understanding and Predicting Weight Loss with Mobile Social Networking Data | 0.551\n",
            "Closed-form Machine Unlearning for Matrix Factorization | 0.523\n",
            "vec2Link: Unifying Heterogeneous Data for Social Link Prediction | 0.534\n",
            "Can Adversarial Weight Perturbations Inject Neural Backdoors | 0.502\n",
            "Minimizing Spectral Radius of Non-Backtracking Matrix by Edge Removal | 0.537\n",
            "Synergizing Local and Global Models for Matrix Approximation | 0.541\n",
            "Does Adversarial Oversampling Help us? | 0.505\n",
            "Convolution-Consistent Collective Matrix Completion | 0.524\n",
            "Towards Understanding of Deepfake Videos in the Wild | 0.521\n",
            "Mining Reaction and Diffusion Dynamics in Social Activities | 0.521\n",
            "Taming Social Bots: Detection, Exploration and Measurement | 0.521\n",
            "Type Theory as a Unifying Paradigm for Modern Databases | 0.530\n",
            "Social Graph Transformer Networks for Pedestrian Trajectory Prediction in Complex Social Scenarios | 0.500\n",
            "Challenges and Solutions to the Student Dropout Prediction Problem in Online Courses | 0.577\n",
            "GALGO: Scalable Graph Analytics with a Parallel DBMS | 0.505\n",
            "Unsupervised Large-Scale Social Network Alignment via Cross Network Embedding | 0.529\n",
            "LatentVis: Investigating and Comparing Variational Auto-Encoders via Their Latent Space | 0.521\n",
            "Improving Low-Rank Matrix Completion with Self-Expressiveness | 0.514\n",
            "An Empirical Study of Community Overlap: Ground-truth, Algorithmic Solutions, and Implications | 0.509\n",
            "Geometric Matrix Completion via Sylvester Multi-Graph Neural Network | 0.515\n",
            "Geometric Estimation of Specificity within Embedding Spaces | 0.527\n",
            "When Structure Meets Keywords: Cohesive Attributed Community Search | 0.511\n",
            "Semi-Supervised Collaborative Learning for Social Spammer and Spam Message Detection in Microblogging | 0.530\n",
            "Optimizing Optimizers: Regret-optimal gradient descent algorithms | 0.620\n",
            "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning | 0.507\n",
            "Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation | 0.536\n",
            "Vortices Instead of Equilibria in MinMax Optimization: Chaos and Butterfly Effects of Online Learning in Zero-Sum Games | 0.517\n",
            "Accelerating Stochastic Gradient Descent | 0.517\n",
            "On the Ability of Neural Nets to Express Distributions | 0.533\n",
            "Rank-one matrix estimation: analytic time evolution of gradient descent dynamics | 0.535\n",
            "A Rank-1 Sketch for Matrix Multiplicative Weights | 0.517\n",
            "Matrix Completion from $O(n)$ Samples in Linear Time | 0.516\n",
            "The Implicit Bias of Benign Overfitting | 0.617\n",
            "How to trap a gradient flow | 0.567\n",
            "Gradient descent follows the regularization path for general losses | 0.556\n",
            "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations | 0.567\n",
            "Neural Networks can Learn Representations with Gradient Descent | 0.509\n",
            "Sample-Optimal Low-Rank Approximation of Distance Matrices | 0.501\n",
            "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon | 0.525\n",
            "Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales | 0.514\n",
            "Near-Optimal Algorithms for Minimax Optimization | 0.504\n",
            "Winnowing with Gradient Descent | 0.548\n",
            "Adaptive Bandit Convex Optimization with Heterogeneous Curvature | 0.518\n",
            "Benign Overfitting of Constant-Stepsize SGD for Linear Regression | 0.507\n",
            "Reasoning About Generalization via Conditional Mutual Information | 0.580\n",
            "Zeroth-order Optimization with Weak Dimension Dependency | 0.518\n",
            "Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD | 0.503\n",
            "Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing | 0.560\n",
            "Causal Matrix Completion | 0.534\n",
            "Be Adaptive, Avoid Overcommitting | 0.503\n",
            "The Summation-Truncation Hybrid: Reusing Discarded Bits for Free | 0.526\n",
            "Improving Generalization with Domain Convex Game | 0.544\n",
            "The Incremental Multiresolution Matrix Factorization Algorithm | 0.521\n",
            "Localized Adversarial Domain Generalization | 0.513\n",
            "Diverse Branch Block: Building a Convolution as an Inception-like Unit | 0.508\n",
            "Regularizing Neural Networks via Minimizing Hyperspherical Energy | 0.501\n",
            "HINT: Hierarchical Neuron Concept Explainer | 0.526\n",
            "Generalist: Decoupling Natural and Robust Generalization | 0.658\n",
            "What It Thinks Is Important Is Important: Robustness Transfers Through Input Gradients | 0.503\n",
            "Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization | 0.539\n",
            "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs | 0.505\n",
            "Generative Flows with Invertible Attentions | 0.506\n",
            "Deep vanishing point detection: Geometric priors make dataset variations vanish | 0.519\n",
            "ReSprop: Reuse Sparsified Backpropagation | 0.588\n",
            "Architectural Backdoors in Neural Networks | 0.533\n",
            "Reflection Removal Using Low-Rank Matrix Completion | 0.546\n",
            "There and Back Again: Revisiting Backpropagation Saliency Methods | 0.590\n",
            "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization | 0.501\n",
            "Online High Rank Matrix Completion | 0.521\n",
            "Robust Combination of Distributed Gradients Under Adversarial Perturbations | 0.546\n",
            "Strike (With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects | 0.517\n",
            "PIEs: Pose Invariant Embeddings | 0.513\n",
            "Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation | 0.503\n",
            "Estimating Example Difficulty using Variance of Gradients | 0.506\n",
            "Kubric: A scalable dataset generator | 0.557\n",
            "Towards Principled Disentanglement for Domain Generalization | 0.552\n",
            "Federated Domain Generalization with Generalization Adjustment | 0.566\n",
            "Failure Modes of Domain Generalization Algorithms | 0.564\n",
            "Regularizing Neural Networks via Adversarial Model Perturbation | 0.524\n",
            "Adaptive Methods for Real-World Domain Generalization | 0.545\n",
            "Towards Global Explanations of Convolutional Neural Networks With Concept Attribution | 0.537\n",
            "Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models | 0.517\n",
            "Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary | 0.518\n",
            "Guaranteed Matrix Completion Under Multiple Linear Transformations | 0.592\n",
            "Convolutional Neural Networks Can Be Deceived by Visual Illusions | 0.510\n",
            "Online Convolutional Reparameterization | 0.524\n",
            "Deep Unlearning via Randomized Conditionally Independent Hessians | 0.507\n",
            "DeepFake Disrupter: The Detector of DeepFake Is My Friend | 0.544\n",
            "Brain-inspired Multilayer Perceptron with Spiking Neurons | 0.605\n",
            "Tangent Space Backpropagation for 3D Transformation Groups | 0.534\n",
            "Fixed-Point Back-Propagation Training | 0.501\n",
            "Alleviation of Gradient Exploding in GANs: Fake Can Be Real | 0.594\n",
            "Learning Not to Learn: Training Deep Neural Networks With Biased Data | 0.514\n",
            "An End-To-End Network for Generating Social Relationship Graphs | 0.530\n",
            "Retina-Like Visual Image Reconstruction via Spiking Neural Model | 0.534\n",
            "Hyperbolic Image Embeddings | 0.509\n",
            "On the Regularization Properties of Structured Dropout | 0.521\n",
            "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation | 0.514\n",
            "Ensembling Off-the-shelf Models for GAN Training | 0.506\n",
            "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives | 0.503\n",
            "Integral Neural Networks | 0.504\n",
            "RES-PCA: A Scalable Approach to Recovering Low-Rank Matrices | 0.587\n",
            "Prompt Consistency for Zero-Shot Task Generalization | 0.502\n",
            "Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training | 0.528\n",
            "Modeling the Music Genre Perception across Language-Bound Cultures | 0.514\n",
            "Unraveling Feature Extraction Mechanisms in Neural Networks | 0.512\n",
            "Interpreting Embedding Spaces by Conceptualization | 0.527\n",
            "Break it Down for Me: A Study in Automated Lyric Annotation | 0.521\n",
            "On the Transformation of Latent Space in Fine-Tuned NLP Models | 0.570\n",
            "The Curious Case of Absolute Position Embeddings | 0.614\n",
            "Charmanteau: Character Embedding Models For Portmanteau Creation | 0.507\n",
            "Attention is not not Explanation | 0.560\n",
            "Rotate King to get Queen: Word Relationships as Orthogonal Transformations in Embedding Space | 0.532\n",
            "Multi-Range Supported Oblivious RAM for Efficient Block Data Retrieval | 0.504\n",
            "CoFrame: A System for Training Novice Cabot Programmers | 0.540\n",
            "Playing the Blame Game with Robots | 0.545\n",
            "“I think you are doing a bad job!” : The Effect of Blame Attribution by a Robot in Human-Robot Collaboration | 0.517\n",
            "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization | 0.583\n",
            "Essential Matrix Estimation using Convex Relaxations in Orthogonal Space | 0.538\n",
            "Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization Without Accessing Target Domain Data | 0.510\n",
            "Generating Attribution Maps with Disentangled Masked Backpropagation | 0.554\n",
            "Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks | 0.512\n",
            "RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks | 0.516\n",
            "Learning Compatible Embeddings | 0.578\n",
            "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks | 0.583\n",
            "Attention on Attention for Image Captioning | 0.526\n",
            "Sketch Your Own GAN | 0.564\n",
            "Fooling Network Interpretation in Image Classification | 0.502\n",
            "Social Diffusion: Long-term Multiple Human Motion Anticipation | 0.532\n",
            "Membrane Potential Batch Normalization for Spiking Neural Networks | 0.523\n",
            "Inherent Redundancy in Spiking Neural Networks | 0.523\n",
            "Seeing What a GAN Cannot Generate | 0.560\n",
            "Unleashing the Potential of Spiking Neural Networks with Dynamic Confidence | 0.562\n",
            "Curriculum Dropout | 0.561\n",
            "Aligning Latent and Image Spaces to Connect the Unconnectable | 0.575\n",
            "Image2song: Song Retrieval via Bridging Image Content and Lyric Words | 0.503\n",
            "Deeper, Broader and Artier Domain Generalization | 0.512\n",
            "Meta Gradient Adversarial Attack | 0.548\n",
            "Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation | 0.511\n",
            "Training Deep Networks to be Spatially Sensitive | 0.501\n",
            "Backpropagation Path Search On Adversarial Transferability | 0.546\n",
            "SSF: Accelerating Training of Spiking Neural Networks with Stabilized Spiking Flow | 0.504\n",
            "Understanding Deep Networks via Extremal Perturbations and Smooth Masks | 0.539\n",
            "Explaining Neural Networks Semantically and Quantitatively | 0.532\n",
            "Sparse and Imperceivable Adversarial Attacks | 0.513\n",
            "Deep Directly-Trained Spiking Neural Networks for Object Detection | 0.530\n",
            "CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks | 0.533\n",
            "iDAG: Invariant DAG Searching for Domain Generalization | 0.503\n",
            "Gradient Estimators for Implicit Models | 0.544\n",
            "A new dog learns old tricks: RL finds classic optimization algorithms | 0.526\n",
            "Learning representations for binary-classification without backpropagation | 0.529\n",
            "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability | 0.530\n",
            "Adaptive Gradient Methods with Dynamic Bound of Learning Rate | 0.538\n",
            "Implicit Gradient Regularization | 0.612\n",
            "An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality | 0.507\n",
            "Convergence of Gradient Methods on Bilinear Zero-Sum Games | 0.501\n",
            "Contextual Dropout: An Efficient Sample-Dependent Dropout Module | 0.552\n",
            "Gradient Origin Networks | 0.598\n",
            "A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima | 0.532\n",
            "Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting | 0.520\n",
            "Adversarial Unlearning of Backdoors via Implicit Hypergradient | 0.510\n",
            "Agnostic Learning of General ReLU Activation Using Gradient Descent | 0.501\n",
            "Slimmable Neural Networks | 0.503\n",
            "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks | 0.524\n",
            "A closer look at the approximation capabilities of neural networks | 0.573\n",
            "Learning Discrete Weights Using the Local Reparameterization Trick | 0.505\n",
            "Gradient Information Matters in Policy Optimization by Back-propagating through Model | 0.519\n",
            "Contrastive Divergence Learning is a Time Reversal Adversarial Game | 0.523\n",
            "Genetic Policy Optimization | 0.517\n",
            "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks | 0.530\n",
            "Spiking Convolutional Neural Networks for Text Classification | 0.540\n",
            "Backpropagation at the Infinitesimal Inference Limit of Energy-Based Models: Unifying Predictive Coding, Equilibrium Propagation, and Contrastive Hebbian Learning | 0.572\n",
            "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | 0.515\n",
            "What Can Neural Networks Reason About? | 0.613\n",
            "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization | 0.542\n",
            "Backstepping Temporal Difference Learning | 0.509\n",
            "Finite Depth and Width Corrections to the Neural Tangent Kernel | 0.524\n",
            "Deep Kernel Machines via the Kernel Reparametrization Trick | 0.501\n",
            "Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning | 0.527\n",
            "Systematic generalisation with group invariant predictions | 0.582\n",
            "Learning A Minimax Optimizer: A Pilot Study | 0.566\n",
            "Finding Actual Descent Directions for Adversarial Training | 0.526\n",
            "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks | 0.545\n",
            "Rethinking Attention with Performers | 0.568\n",
            "Fraternal Dropout | 0.625\n",
            "Are GAN Generated Images Easy to Detect? A Critical Analysis of the State-Of-The-Art | 0.506\n",
            "Interpret The Predictions Of Deep Networks Via Re-Label Distillation | 0.507\n",
            "Using simplified chords sequences to classify songs genres | 0.519\n",
            "Efficient low rank matrix approximation via orthogonality pursuit and ℓ2 regularization | 0.509\n",
            "Top attention in line with time: A light-weight strategy | 0.566\n",
            "The Road to Live Programming: Insights from the Practice | 0.519\n",
            "Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code | 0.507\n",
            "Programming Not Only by Example | 0.507\n",
            "Diversity-Driven Automated Formal Verification | 0.501\n",
            "A secure and verifiable outsourcing scheme for matrix inverse computation | 0.553\n",
            "Fast Generation-Based Gradient Leakage Attacks against Highly Compressed Gradients | 0.560\n",
            "Challenging the limits: Sampling online social networks with cost constraints | 0.503\n",
            "Robustified Learning for Online Optimization with Memory Costs | 0.534\n",
            "BlitzG: Exploiting high-bandwidth networks for fast graph processing | 0.554\n",
            "Denoising and deinterleaving of EPSI data using structured low-rank matrix recovery | 0.504\n",
            "Hybrid Spiking Neural Networks Fine-Tuning for Hippocampus Segmentation | 0.582\n",
            "Constrain Latent Space for Schizophrenia Classification via Dual Space Mapping Net | 0.500\n",
            "Session details: Session 6B: GPGPU | 0.536\n",
            "Morphable Counters: Enabling Compact Integrity Trees For Low-Overhead Secure Memories | 0.563\n",
            "Practical Byte-Granular Memory Blacklisting using Califorms | 0.526\n",
            "Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions by Exploiting DRAM Data Retention Characteristics | 0.505\n",
            "Stack Bounds Protection with Low Fat Pointers | 0.566\n",
            "The Importance of Communities for Learning to Influence | 0.519\n",
            "Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks | 0.538\n",
            "Backpropagation-Friendly Eigendecomposition | 0.575\n",
            "A New Theory for Matrix Completion | 0.609\n",
            "Online Linear Optimization with Many Hints | 0.504\n",
            "The Sparse Manifold Transform | 0.536\n",
            "Rethinking gradient sparsification as total error minimization | 0.588\n",
            "Rank-1 Matrix Completion with Gradient Descent and Small Random Initialization | 0.559\n",
            "The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks | 0.501\n",
            "Attention-Gated Brain Propagation: How the brain can implement reward-based error backpropagation | 0.505\n",
            "Gradient Estimation with Stochastic Softmax Tricks | 0.540\n",
            "The Pitfalls of Simplicity Bias in Neural Networks | 0.555\n",
            "Necessary and Sufficient Geometries for Gradient Methods | 0.570\n",
            "Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model | 0.530\n",
            "Zeroth-Order Negative Curvature Finding: Escaping Saddle Points without Gradients | 0.519\n",
            "Bridging Discrete and Backpropagation: Straight-Through and Beyond | 0.713\n",
            "Gradient Episodic Memory for Continual Learning | 0.511\n",
            "Concrete Dropout | 0.576\n",
            "Matrix Norm Estimation from a Few Entries | 0.518\n",
            "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration | 0.536\n",
            "Nonsmooth Implicit Differentiation for Machine Learning and Optimization | 0.504\n",
            "Proximity Operator of the Matrix Perspective Function and its Applications | 0.588\n",
            "Convergence of Adversarial Training in Overparametrized Neural Networks | 0.533\n",
            "Matrix Completion with Hierarchical Graph Side Information | 0.501\n",
            "Exploitability Minimization in Games and Beyond | 0.514\n",
            "A Causal View on Robustness of Neural Networks | 0.502\n",
            "Conic Descent and its Application to Memory-efficient Optimization over Positive Semidefinite Matrices | 0.526\n",
            "Early-stopped neural networks are consistent | 0.514\n",
            "Old can be Gold: Better Gradient Flow can Make Vanilla-GCNs Great Again | 0.509\n",
            "Approximation Based Variance Reduction for Reparameterization Gradients | 0.556\n",
            "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients | 0.563\n",
            "A Unifying Framework for Online Optimization with Long-Term Constraints | 0.505\n",
            "Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power | 0.523\n",
            "Attention is All you Need | 0.680\n",
            "DISCO: Adversarial Defense with Local Implicit Functions | 0.513\n",
            "Gradient Descent Can Take Exponential Time to Escape Saddle Points | 0.507\n",
            "Implicit Regularization in Deep Matrix Factorization | 0.534\n",
            "Momentum Centering and Asynchronous Update for Adaptive Gradient Methods | 0.504\n",
            "Probabilistic Tensor Decomposition of Neural Population Spiking Activity | 0.501\n",
            "Weight Agnostic Neural Networks | 0.517\n",
            "Uniform Convergence of Gradients for Non-Convex Learning and Optimization | 0.506\n",
            "Causal Component Analysis | 0.531\n",
            "PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization | 0.518\n",
            "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction | 0.536\n",
            "Prophet Attention: Predicting Attention with Future Attention | 0.519\n",
            "The Limits of Post-Selection Generalization | 0.570\n",
            "Gradient Starvation: A Learning Proclivity in Neural Networks | 0.558\n",
            "On the Local Hessian in Back-propagation | 0.543\n",
            "Input Similarity from the Neural Network Perspective | 0.500\n",
            "Meta-Gradient Reinforcement Learning | 0.526\n",
            "Shadowing Properties of Optimization Algorithms | 0.570\n",
            "A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases | 0.568\n",
            "The Convergence of Sparsified Gradient Methods | 0.578\n",
            "On Convergence and Generalization of Dropout Training | 0.609\n",
            "Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients? | 0.554\n",
            "Mesoscopic modeling of hidden spiking neurons | 0.576\n",
            "On the Dimensionality of Word Embedding | 0.538\n",
            "Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information | 0.519\n",
            "Sinkhorn Barycenter via Functional Gradient Descent | 0.503\n",
            "Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks | 0.502\n",
            "Gradient Flossing: Improving Gradient Descent through Dynamic Control of Jacobians | 0.525\n",
            "Intriguing Properties of Contrastive Losses | 0.510\n",
            "Delving into Sequential Patches for Deepfake Detection | 0.619\n",
            "Training Spiking Neural Networks with Event-driven Backpropagation | 0.543\n",
            "KNG: The K-Norm Gradient Mechanism | 0.509\n",
            "Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax | 0.543\n",
            "DisARM: An Antithetic Gradient Estimator for Binary Latent Variables | 0.504\n",
            "Minimax Estimation of Bandable Precision Matrices | 0.516\n",
            "Deep Residual Learning in Spiking Neural Networks | 0.521\n",
            "On the Value of Infinite Gradients in Variational Autoencoder Models | 0.502\n",
            "Pay attention to your loss : understanding misconceptions about Lipschitz neural networks | 0.510\n",
            "Adapting to Function Difficulty and Growth Conditions in Private Optimization | 0.537\n",
            "Fast and Accurate Stochastic Gradient Estimation | 0.538\n",
            "Fast Attention Requires Bounded Entries | 0.509\n",
            "Quantifying and Improving Transferability in Domain Generalization | 0.539\n",
            "Fast Pure Exploration via Frank-Wolfe | 0.505\n",
            "A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking | 0.525\n",
            "The Reversible Residual Network: Backpropagation Without Storing Activations | 0.641\n",
            "Implicit Sparse Regularization: The Impact of Depth and Early Stopping | 0.514\n",
            "On the Importance of Gradients for Detecting Distributional Shifts in the Wild | 0.502\n",
            "Network-to-Network Regularization: Enforcing Occam's Razor to Improve Generalization | 0.584\n",
            "Mixture Matrix Completion | 0.547\n",
            "Learning Hard Optimization Problems: A Data Generation Perspective | 0.538\n",
            "How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods | 0.503\n",
            "Online Matrix Completion with Side Information | 0.529\n",
            "Understanding spiking networks through convex optimization | 0.553\n",
            "Set Prediction in the Latent Space | 0.588\n",
            "Matrix Compression via Randomized Low Rank and Low Precision Factorization | 0.510\n",
            "Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization | 0.539\n",
            "Hyperparameter Optimization Is Deceiving Us, and How to Stop It | 0.529\n",
            "Conditioning and Processing: Techniques to Improve Information-Theoretic Generalization Bounds | 0.504\n",
            "Robust Generalization despite Distribution Shift via Minimum Discriminating Information | 0.521\n",
            "The Numerics of GANs | 0.516\n",
            "Stochastic Bias-Reduced Gradient Methods | 0.532\n",
            "A comprehensive analysis on attention models | 0.520\n",
            "On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective | 0.547\n",
            "Backpropagation with Callbacks: Foundations for Efficient and Expressive Differentiable Programming | 0.548\n",
            "SWAD: Domain Generalization by Seeking Flat Minima | 0.509\n",
            "From Gradient Flow on Population Loss to Learning with Stochastic Gradient Descent | 0.523\n",
            "Reproducibility in Optimization: Theoretical Framework and Limits | 0.565\n",
            "Sinkhorn Natural Gradient for Generative Models | 0.524\n",
            "Domain Generalization via Entropy Regularization | 0.512\n",
            "Regret Minimization via Saddle Point Optimization | 0.535\n",
            "On the Inductive Bias of Neural Tangent Kernels | 0.520\n",
            "Evolving Connectivity for Recurrent Spiking Neural Networks | 0.500\n",
            "An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution | 0.510\n",
            "R-Drop: Regularized Dropout for Neural Networks | 0.502\n",
            "Discovered Policy Optimisation | 0.503\n",
            "Bayesian Optimization with Gradients | 0.512\n",
            "The Expressive Power of Neural Networks: A View from the Width | 0.509\n",
            "Bayesian GAN | 0.526\n",
            "Biologically Inspired Dynamic Thresholds for Spiking Neural Networks | 0.572\n",
            "Deep Leakage from Gradients | 0.703\n",
            "Reducing Reparameterization Gradient Variance | 0.533\n",
            "Efficiently avoiding saddle points with zero order methods: No gradients required | 0.508\n",
            "Scalable Interpretability via Polynomials | 0.503\n",
            "The Lingering of Gradients: How to Reuse Gradients Over Time | 0.627\n",
            "Open Graph Benchmark: Datasets for Machine Learning on Graphs | 0.504\n",
            "Controlled Sparsity via Constrained Optimization or: How I Learned to Stop Tuning Penalties and Love Constraints | 0.532\n",
            "Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization | 0.501\n",
            "Gradient Dynamics of Shallow Univariate ReLU Networks | 0.518\n",
            "Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness | 0.526\n",
            "Regularized linear autoencoders recover the principal components, eventually | 0.516\n",
            "Maximum Mean Discrepancy Gradient Flow | 0.582\n",
            "Overfitting Can Be Harmless for Basis Pursuit, But Only to a Degree | 0.529\n",
            "Numerical influence of ReLU'(0) on backpropagation | 0.559\n",
            "Towards a Theoretical Framework of Out-of-Distribution Generalization | 0.559\n",
            "ZooD: Exploiting Model Zoo for Out-of-Distribution Generalization | 0.518\n",
            "Dynamic Inverse Reinforcement Learning for Characterizing Animal Behavior | 0.511\n",
            "On the Overlooked Structure of Stochastic Gradients | 0.563\n",
            "Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions | 0.525\n",
            "On Blame Attribution for Accountable Multi-Agent Sequential Decision Making | 0.534\n",
            "Implicit Bias of Gradient Descent on Linear Convolutional Networks | 0.509\n",
            "Most Neural Networks Are Almost Learnable | 0.607\n",
            "Model-Based Domain Generalization | 0.525\n",
            "GAIT-prop: A biologically plausible learning rule derived from backpropagation of error | 0.520\n",
            "In Search of Robust Measures of Generalization | 0.519\n",
            "Visualizing the Loss Landscape of Neural Nets | 0.505\n",
            "Theoretically Provable Spiking Neural Networks | 0.566\n",
            "On the Role of Optimization in Double Descent: A Least Squares Study | 0.506\n",
            "Uncovering Meanings of Embeddings via Partial Orthogonality | 0.583\n",
            "A unified framework for information-theoretic generalization bounds | 0.521\n",
            "An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild | 0.531\n",
            "IM-Loss: Information Maximization Loss for Spiking Neural Networks | 0.502\n",
            "Understanding Deep Gradient Leakage via Inversion Influence Functions | 0.608\n",
            "Generalized Inverse Optimization through Online Learning | 0.517\n",
            "Neural Networks with Cheap Differential Operators | 0.507\n",
            "Neural Similarity Learning | 0.502\n",
            "What the Vec? Towards Probabilistically Grounded Embeddings | 0.537\n",
            "Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization | 0.543\n",
            "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality | 0.534\n",
            "Stein Variational Gradient Descent as Gradient Flow | 0.557\n",
            "The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization | 0.547\n",
            "SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks | 0.513\n",
            "Meta Learning Backpropagation And Improving It | 0.628\n",
            "MorphTE: Injecting Morphology in Tensorized Embeddings | 0.505\n",
            "Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time | 0.526\n",
            "Neural Tangent Kernel: Convergence and Generalization in Neural Networks | 0.523\n",
            "Robust Optimization for Non-Convex Objectives | 0.514\n",
            "Dynamic Regret of Policy Optimization in Non-stationary Environments | 0.522\n",
            "Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks | 0.506\n",
            "Fast Projected Newton-like Method for Precision Matrix Estimation under Total Positivity | 0.533\n",
            "Benign, Tempered, or Catastrophic: Toward a Refined Taxonomy of Overfitting | 0.644\n",
            "Dual Adaptivity: A Universal Algorithm for Minimizing the Adaptive Regret of Convex Functions | 0.503\n",
            "A Causal Analysis of Harm | 0.522\n",
            "The Curse of Unrolling: Rate of Differentiating Through Optimization | 0.580\n",
            "Latent SDEs on Homogeneous Spaces | 0.545\n",
            "Reparameterizing Mirror Descent as Gradient Descent | 0.525\n",
            "Adversarially Robust Generalization Requires More Data | 0.532\n",
            "Long short-term memory and Learning-to-learn in networks of spiking neurons | 0.545\n",
            "On Blackbox Backpropagation and Jacobian Sensing | 0.539\n",
            "Scalable Global Optimization via Local Bayesian Optimization | 0.506\n",
            "Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing | 0.553\n",
            "An In-depth Study of Stochastic Backpropagation | 0.648\n",
            "Trivializations for Gradient-Based Optimization on Manifolds | 0.576\n",
            "Auto Learning Attention | 0.508\n",
            "Rethinking the Backward Propagation for Adversarial Transferability | 0.507\n",
            "A theory on the absence of spurious optimality | 0.504\n",
            "All Word Embeddings from One Embedding | 0.502\n",
            "Gradient Information for Representation and Modeling | 0.551\n",
            "Improving Neural Ordinary Differential Equations with Nesterov's Accelerated Gradient Method | 0.503\n",
            "Learning Latent Subspaces in Variational Autoencoders | 0.510\n",
            "Biologically-plausible backpropagation through arbitrary timespans via local neuromodulators | 0.538\n",
            "Decentralized Matrix Sensing: Statistical Guarantees and Fast Convergence | 0.517\n",
            "Online Optimization with Memory and Competitive Control | 0.548\n",
            "SnAKe: Bayesian Optimization with Pathwise Exploration | 0.513\n",
            "Gradient Descent: The Ultimate Optimizer | 0.606\n",
            "Eigen-Distortions of Hierarchical Representations | 0.512\n",
            "Data-Driven Conditional Robust Optimization | 0.504\n",
            "Exact natural gradient in deep linear networks and its application to the nonlinear case | 0.558\n",
            "A Domain Agnostic Measure for Monitoring and Evaluating GANs | 0.534\n",
            "Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel | 0.509\n",
            "Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model | 0.559\n",
            "VarGrad: A Low-Variance Gradient Estimator for Variational Inference | 0.502\n",
            "Instance based Generalization in Reinforcement Learning | 0.528\n",
            "Triple descent and the two kinds of overfitting: where and why do they appear? | 0.510\n",
            "Lower Bounds on Adaptive Sensing for Matrix Recovery | 0.563\n",
            "Wide neural networks of any depth evolve as linear models under gradient descent | 0.524\n",
            "From Boltzmann Machines to Neural Networks and Back Again | 0.565\n",
            "Homomorphic Matrix Completion | 0.517\n",
            "Meta-Learning with Implicit Gradients | 0.553\n",
            "Understanding Benign Overfitting in Gradient-Based Meta Learning | 0.520\n",
            "Slice Sampling Reparameterization Gradients | 0.531\n",
            "Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows | 0.515\n",
            "DAC-DETR: Divide the Attention Layers and Conquer | 0.529\n",
            "Rank Overspecified Robust Matrix Recovery: Subgradient Method and Exact Recovery | 0.594\n",
            "Global Convergence of Gradient Descent for Deep Linear Residual Networks | 0.521\n",
            "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks | 0.535\n",
            "Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients | 0.547\n",
            "Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel | 0.507\n",
            "Implicit Reparameterization Gradients | 0.611\n",
            "Approximate Feature Collisions in Neural Nets | 0.532\n",
            "Sparse Spiking Gradient Descent | 0.545\n",
            "Large-Scale Wasserstein Gradient Flows | 0.503\n",
            "Learning where to learn: Gradient sparsity in meta and continual learning | 0.541\n",
            "Distilled Gradient Aggregation: Purify Features for Input Attribution in the Deep Neural Network | 0.525\n",
            "Local Convergence of Gradient Methods for Min-Max Games: Partial Curvature Generically Suffices | 0.552\n",
            "Efficient and Accurate Gradients for Neural SDEs | 0.519\n",
            "Are Sixteen Heads Really Better than One? | 0.699\n",
            "Explicit loss asymptotics in the gradient descent training of neural networks | 0.537\n",
            "Cost efficient gradient boosting | 0.543\n",
            "Tight Analysis of Extra-gradient and Optimistic Gradient Methods For Nonconvex Minimax Problems | 0.525\n",
            "Implicit Regularization in Matrix Sensing via Mirror Descent | 0.587\n",
            "HOUDINI: Lifelong Learning as Program Synthesis | 0.530\n",
            "Geometric Analysis of Matrix Sensing over Graphs | 0.544\n",
            "Online Training Through Time for Spiking Neural Networks | 0.507\n",
            "Can the Brain Do Backpropagation? - Exact Implementation of Backpropagation in Predictive Coding Networks | 0.540\n",
            "On Certified Generalization in Structured Prediction | 0.551\n",
            "Emergence of Hierarchical Layers in a Single Sheet of Self-Organizing Spiking Neurons | 0.565\n",
            "Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks | 0.561\n",
            "Gradient Descent for Spiking Neural Networks | 0.527\n",
            "Backpropagating Linearly Improves Transferability of Adversarial Examples | 0.548\n",
            "Large Scale Structure of Neural Network Loss Landscapes | 0.554\n",
            "Competitive Gradient Descent | 0.570\n",
            "Preventing Gradient Explosions in Gated Recurrent Units | 0.584\n",
            "On Calibration and Out-of-domain Generalization | 0.504\n",
            "Limits to Depth Efficiencies of Self-Attention | 0.514\n",
            "A Meta-Analysis of Overfitting in Machine Learning | 0.591\n",
            "Implicit Regularization of Discrete Gradient Dynamics in Deep Linear Neural Networks | 0.532\n",
            "Separable explanations of neural network decisions | 0.542\n",
            "Robust Matrix Sensing in the Semi-Random Model | 0.532\n",
            "Online Convex Matrix Factorization with Representative Regions | 0.521\n",
            "How To Make the Gradients Small Stochastically | 0.580\n",
            "DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation | 0.504\n",
            "Emergent Communication of Generalizations | 0.631\n",
            "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks | 0.513\n",
            "A Catalyst Framework for Minimax Optimization | 0.561\n",
            "Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective | 0.558\n",
            "GUST: Combinatorial Generalization by Unsupervised Grouping with Neuronal Coherence | 0.541\n",
            "Are GANs Created Equal? A Large-Scale Study | 0.516\n",
            "From Tempered to Benign Overfitting in ReLU Neural Networks | 0.502\n",
            "Evolved Policy Gradients | 0.527\n",
            "Towards a Unified Information-Theoretic Framework for Generalization | 0.639\n",
            "A Gradient Method for Multilevel Optimization | 0.506\n",
            "Plug in estimation in high dimensional linear inverse problems a rigorous analysis | 0.506\n",
            "Convergence and Alignment of Gradient Descentwith Random Back propagation Weights | 0.557\n",
            "Concept Embedding Models | 0.520\n",
            "Online Convex Optimization with Hard Constraints: Towards the Best of Two Worlds and Beyond | 0.504\n",
            "Teaching a GAN What Not to Learn | 0.509\n",
            "Multi-view Matrix Factorization for Linear Dynamical System Estimation | 0.507\n",
            "Neural Nearest Neighbors Networks | 0.515\n",
            "Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies | 0.592\n",
            "The Wasserstein Proximal Gradient Algorithm | 0.536\n",
            "Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates | 0.531\n",
            "Adaptive optimal training of animal behavior | 0.531\n",
            "Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms | 0.529\n",
            "Training Spiking Neural Networks with Local Tandem Learning | 0.551\n",
            "Exploring Generalization in Deep Learning | 0.527\n",
            "How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery? | 0.507\n",
            "Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems | 0.524\n",
            "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry | 0.507\n",
            "Partial Matrix Completion | 0.599\n",
            "Conformalized matrix completion | 0.641\n",
            "Detecting Overfitting via Adversarial Examples | 0.600\n",
            "How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD | 0.537\n",
            "Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck | 0.523\n",
            "Domain Generalization without Excess Empirical Risk | 0.515\n",
            "Latent Space Translation via Semantic Alignment | 0.589\n",
            "Information-constrained optimization: can adaptive processing of gradients help? | 0.537\n",
            "Constant Regret, Generalized Mixability, and Mirror Descent | 0.502\n",
            "Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent | 0.515\n",
            "Optimistic Meta-Gradients | 0.623\n",
            "Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies | 0.520\n",
            "Scalable Robust Matrix Factorization with Nonconvex Loss | 0.513\n",
            "Algorithmic Instabilities of Accelerated Gradient Descent | 0.511\n",
            "On Numerosity of Deep Neural Networks | 0.516\n",
            "GAN Memory with No Forgetting | 0.510\n",
            "Computational Separations between Sampling and Optimization | 0.508\n",
            "Optimization over Continuous and Multi-dimensional Decisions with Observational Data | 0.507\n",
            "An Exploration-by-Optimization Approach to Best of Both Worlds in Linear Bandits | 0.531\n",
            "Surprising Instabilities in Training Deep Networks and a Theoretical Analysis | 0.552\n",
            "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization | 0.501\n",
            "Backprop with Approximate Activations for Memory-efficient Network Training | 0.519\n",
            "Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons | 0.511\n",
            "Efficient Neural Network Training via Forward and Backward Propagation Sparsification | 0.511\n",
            "Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms | 0.501\n",
            "Lost in Latent Space: Examining failures of disentangled models at combinatorial generalisation | 0.524\n",
            "Optimal Gradient-based Algorithms for Non-concave Bandit Optimization | 0.530\n",
            "Black-Box Generalization | 0.667\n",
            "Chaining Mutual Information and Tightening Generalization Bounds | 0.555\n",
            "Fast Axiomatic Attribution for Neural Networks | 0.540\n",
            "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel | 0.502\n",
            "Truly Deterministic Policy Optimization | 0.533\n",
            "Towards Better Generalization of Adaptive Gradient Methods | 0.557\n",
            "On the Power and Limitations of Random Features for Understanding Neural Networks | 0.501\n",
            "Gradient Boosted Normalizing Flows | 0.523\n",
            "Context Selection for Embedding Models | 0.501\n",
            "Biologically plausible solutions for spiking networks with efficient coding | 0.590\n",
            "\"Why Not Other Classes?\": Towards Class-Contrastive Back-Propagation Explanations | 0.540\n",
            "Mapping spiking neural networks onto a manycore neuromorphic architecture | 0.550\n",
            "FreezeML: complete and easy type inference for first-class polymorphism | 0.588\n",
            "PLDI '22: 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation, San Diego, CA, USA, June 13 - 17, 2022 | 0.507\n",
            "Toward efficient gradual typing for structural types via coercions | 0.510\n",
            "Low-latency, high-throughput garbage collection | 0.562\n",
            "Universally Composable $\\varSigma $-protocols in the Global Random-Oracle Model | 0.545\n",
            "A Black-Box Construction of Fully-Simulatable, Round-Optimal Oblivious Transfer from Strongly Uniform Key Agreement | 0.503\n",
            "Beyond MPC-in-the-Head: Black-Box Constructions of Short Zero-Knowledge Proofs | 0.505\n",
            "Zero Knowledge Protocols from Succinct Constraint Detection | 0.517\n",
            "Lessons Learned from the Chameleon Testbed | 0.729\n",
            "Alleviating Garbage Collection Interference Through Spatial Separation in All Flash Arrays | 0.539\n",
            "Uniform Embeddings for Robinson Similarity Matrices | 0.561\n",
            "A High-Performance Graph Engine for Efficient Social Network Analysis | 0.669\n",
            "Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018 | 0.575\n",
            "Sampling from Social Networks with Attributes | 0.548\n",
            "An Army of Me: Sockpuppets in Online Discussion Communities | 0.535\n",
            "Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017 | 0.592\n",
            "Cookie Synchronization: Everything You Always Wanted to Know But Were Afraid to Ask | 0.594\n",
            "Worldwide Universities Network (WUN) Web Observatory: Applying Lessons from the Web to Transform the Research Data Ecosystem | 0.517\n",
            "Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018, Lyon, France, April 23-27, 2018 | 0.610\n",
            "Lightweight source localization for large-scale social networks | 0.533\n",
            "RaRE: Social Rank Regulated Large-scale Network Embedding | 0.540\n",
            "Proceedings of the 26th International Conference on World Wide Web | 0.598\n",
            "Constructing Knowledge Graph for Social Networks in A Deep and Holistic Way | 0.528\n",
            "Generating Simple Directed Social Network Graphs for Information Spreading | 0.547\n",
            "12th Temporal Web Analytics Workshop (TempWeb) Overview | 0.501\n",
            "Editorial: 26th International World Wide Web Conference, WWW 2017 Proceedings | 0.554\n",
            "NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors | 0.522\n",
            "Through the Lens of the Web Conference Series: A Look Into the History of the Web | 0.636\n",
            "13th Temporal Web Analytics Workshop (TempWeb) Overview | 0.506\n",
            "Can Deepfakes be created on a whim? | 0.533\n",
            "A History of Diversity in The Web (Conference) | 0.512\n",
            "Iterative Knowledge Extraction from Social Networks | 0.503\n",
            "Estimate the Implicit Likelihoods of GANs with Application to Anomaly Detection | 0.527\n",
            "A Scalable, Adaptive and Sound Nonconvex Regularizer for Low-rank Matrix Completion | 0.535\n",
            "RAQ: Relationship-Aware Graph Querying in Large Networks | 0.506\n",
            "Web Science @ 10 | 0.529\n",
            "Shut Up and Run: the Never-ending Quest for Social Fitness | 0.616\n",
            "Proceedings of the 26th International Conference on World Wide Web | 0.598\n",
            "Deep Graph Clustering in Social Network | 0.550\n",
            "The Capable Web | 0.532\n",
            "MOOC Dropout Prediction: Lessons Learned from Making Pipelines Interpretable | 0.525\n",
            "“Way back then”: A Data-driven View of 25+ years of Web Evolution | 0.513\n",
            "Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 | 0.507\n",
            "Privacy Preserving Distributed Analysis of Social Networks | 0.508\n",
            "The Web-Wide World | 0.636\n",
            "xGCN: An Extreme Graph Convolutional Network for Large-scale Social Link Prediction | 0.519\n",
            "Graph Neural Networks for Friend Ranking in Large-scale Social Platforms | 0.521\n",
            "Web 3.0: The Future of Internet | 0.521\n",
            "Companion Proceedings of the Web Conference 2021 | 0.563\n",
            "Are Two Heads Better Than One? An Exploration of Ambiguity in Crowd-Collected Language Decisions from the Phrase Detectives Game. | 0.509\n",
            "Cookie Swap Party: Abusing First-Party Cookies for Web Tracking | 0.634\n",
            "Four-Dimensional Shopping Mall: Sequential Group Willingness Optimization under VR Environments | 0.556\n",
            "The World Wide Web Conference | 0.533\n",
            "Twin Peaks, a Model for Recurring Cascades | 0.604\n",
            "“It's almost like they're trying to hide it”: How User-Provided Image Descriptions Have Failed to Make Twitter Accessible | 0.523\n",
            "The One Hundred Year Web | 1.000\n",
            "The Paradigm-Shift of Social Spambots: Evidence, Theories, and Tools for the Arms Race | 0.841\n",
            "Near-Perfect Recovery in the One-Dimensional Latent Space Model | 0.605\n",
            "I Would Not Plant Apple Trees If the World Will Be Wiped: Analyzing Hundreds of Millions of Behavioral Records of Players During an MMORPG Beta Test | 0.547\n",
            "Are All Successful Communities Alike? Characterizing and Predicting the Success of Online Communities | 0.784\n",
            "Fighting Against Deepfake: Patch&Pair Convolutional Neural Networks (PPCNN) | 0.639\n",
            "The Spread of Physical Activity Through Social Networks | 1.000\n",
            "FastSNG: The Fastest Social Network Dataset Generator | 1.000\n",
            "Genre Differences of Song Lyrics and Artist Wikis: An Analysis of Popularity, Length, Repetitiveness, and Readability | 0.687\n",
            "Aspect of Blame in Tweets: A Deep Recurrent Neural Network Approach | 0.590\n",
            "BoFGAN: Towards A New Structure of Backward-or-Forward Generative Adversarial Nets | 0.507\n",
            "FORank: Fast ObjectRank for Large Heterogeneous Graphs | 0.504\n",
            "Proceedings of the 26th International Conference on World Wide Web Companion | 0.565\n",
            "Lessons from neuroscience | 0.559\n",
            "From Reaction to Proaction: Unexplored Ways to the Detection of Evolving Spambots | 0.709\n",
            "A Never-Ending Project for Humanity Called “the Web” | 0.647\n",
            "Understanding Dropout for Graph Neural Networks | 0.559\n",
            "On the Value of Wikipedia as a Gateway to the Web | 0.529\n",
            "Companion of The 2019 World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019 | 0.542\n",
            "Is this the Era of Misinformation yet: Combining Social Bots and Fake News to Deceive the Masses | 0.513\n",
            "781 titles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "There were still some unideal titles found in the output set, so there can still be more improvements made with more time spent into optimizing the filtering. With more time, I would look into trying to have a dataset of bad titles that contain only technical jargon and have the similarity checking aim to get closer with silly embeddings and farther away from similarity with technical-only titles."
      ],
      "metadata": {
        "id": "cvUZXm4O89T6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "942787c928a248bf931936ef633ef04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7192e37c7a604742900d46235acd2200",
              "IPY_MODEL_24bf4077ae7941e391afcb45b19edeb5",
              "IPY_MODEL_d441fdd8080e4f89afa59d2055961b26"
            ],
            "layout": "IPY_MODEL_4d98869e4564453bb104e9f617187213"
          }
        },
        "7192e37c7a604742900d46235acd2200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea059fb25cd44f5970bd4fd289d1311",
            "placeholder": "​",
            "style": "IPY_MODEL_4875f0b0be564f6385161fdfdd7c7436",
            "value": "Batches: 100%"
          }
        },
        "24bf4077ae7941e391afcb45b19edeb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b869373f794c9bb6404d8a789347e0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bff571f59f54a5c86ef889dcb1a6c90",
            "value": 1
          }
        },
        "d441fdd8080e4f89afa59d2055961b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1057878a0c3d42c399bd1cf5193c0d91",
            "placeholder": "​",
            "style": "IPY_MODEL_6b1042898ea44588ad0be772843bbf80",
            "value": " 1/1 [00:00&lt;00:00, 38.83it/s]"
          }
        },
        "4d98869e4564453bb104e9f617187213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea059fb25cd44f5970bd4fd289d1311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4875f0b0be564f6385161fdfdd7c7436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83b869373f794c9bb6404d8a789347e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bff571f59f54a5c86ef889dcb1a6c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1057878a0c3d42c399bd1cf5193c0d91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1042898ea44588ad0be772843bbf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
